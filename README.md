# üå§Ô∏è Weather Data Analytics Platform

A comprehensive, containerized weather data pipeline that orchestrates real-time weather data ingestion, transformation, and visualization using modern data engineering tools.
<div align="center">
  
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Apache-Airflow-red?style=for-the-badge&logo=apache-airflow)
![dbt](https://img.shields.io/badge/dbt-Core-orange?style=for-the-badge&logo=dbt)
![Superset](https://img.shields.io/badge/Superset-Business_Intelligence-20A6C9?style=for-the-badge&logo=apache&logoColor=white)

</div>

## üèóÔ∏è Architecture Overview

This project implements a modern ELT (Extract, Load, Transform) data pipeline for weather data analytics with the following architecture:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Weatherstack  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Airflow      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   PostgreSQL    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Apache        ‚îÇ
‚îÇ      API        ‚îÇ    ‚îÇ  Orchestrator   ‚îÇ    ‚îÇ   Database      ‚îÇ    ‚îÇ  Superset       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ                        ‚îÇ
                                ‚ñº                        ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ      dbt        ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ     Redis       ‚îÇ
                       ‚îÇ  Transformations‚îÇ    ‚îÇ     Cache       ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Key Features

- **üìä Real-time Data Ingestion**: Automated weather data collection from Weatherstack API
- **üîÑ Orchestrated Workflows**: Apache Airflow manages the entire data pipeline
- **üõ†Ô∏è Data Transformation**: dbt handles data modeling and transformations
- **üìà Interactive Dashboards**: Apache Superset for data visualization and analytics
- **üê≥ Containerized Architecture**: Fully dockerized for easy deployment and scaling
- **‚ö° High Performance**: PostgreSQL database with Redis caching
- **üîç Data Quality**: Built-in deduplication and data validation

## üõ†Ô∏è Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | Apache Airflow 3.0.0 | Workflow management and scheduling |
| **Database** | PostgreSQL 14.17 | Primary data storage |
| **Transformation** | dbt (Data Build Tool) | Data modeling and transformations |
| **Visualization** | Apache Superset 3.0.0 | Business intelligence and dashboards |
| **Caching** | Redis 7 | Session management and caching |
| **Containerization** | Docker & Docker Compose | Service orchestration |
| **API Integration** | Weatherstack API | Weather data source |

## üìÅ Project Structure

```
weather_data_project/
‚îú‚îÄ‚îÄ üê≥ docker-compose.yml          # Multi-service container orchestration
‚îú‚îÄ‚îÄ üìã Makefile                    # Build and deployment automation
‚îú‚îÄ‚îÄ 
‚îú‚îÄ‚îÄ üåä airflow/                    # Workflow orchestration
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ       ‚îî‚îÄ‚îÄ orchestrator.py       # Main DAG for data pipeline
‚îÇ
‚îú‚îÄ‚îÄ üîå api-request/                # Data ingestion module
‚îÇ   ‚îú‚îÄ‚îÄ api_request.py            # Weatherstack API client
‚îÇ   ‚îî‚îÄ‚îÄ insert_records.py         # Database insertion logic
‚îÇ
‚îú‚îÄ‚îÄ üîß dbt/                       # Data transformation layer
‚îÇ   ‚îú‚îÄ‚îÄ profiles.yml              # Database connection profiles
‚îÇ   ‚îî‚îÄ‚îÄ weather_project/          # dbt project
‚îÇ       ‚îú‚îÄ‚îÄ dbt_project.yml       # Project configuration
‚îÇ       ‚îî‚îÄ‚îÄ models/               # Data models
‚îÇ           ‚îú‚îÄ‚îÄ sources/          # Source definitions
‚îÇ           ‚îú‚îÄ‚îÄ staging/          # Staging transformations
‚îÇ           ‚îî‚îÄ‚îÄ mart/             # Business logic models
‚îÇ
‚îú‚îÄ‚îÄ üêò postgres/                  # Database initialization
‚îÇ   ‚îú‚îÄ‚îÄ airflow_init.sql          # Airflow database setup
‚îÇ   ‚îî‚îÄ‚îÄ superset_init.sql         # Superset database setup
‚îÇ
‚îî‚îÄ‚îÄ üêã docker/                    # Container configurations
    ‚îú‚îÄ‚îÄ docker-bootstrap.sh       # Superset bootstrap script
    ‚îú‚îÄ‚îÄ docker-init.sh             # Superset initialization
    ‚îî‚îÄ‚îÄ superset_config.py         # Superset configuration
```

## üéØ Data Pipeline Flow

### 1. **Data Ingestion** üì•
- **Source**: Weatherstack API provides real-time weather data for Fez, Morocco
- **Frequency**: Every minute (configurable via Airflow DAG)
- **Format**: JSON response with location and current weather information
- **Storage**: Raw data stored in `dev.raw_weather_data` table

### 2. **Data Orchestration** üéº
- **Tool**: Apache Airflow manages the entire pipeline
- **Schedule**: Automated execution every minute
- **Tasks**:
  - `ingest_data_task`: Fetches and stores raw weather data
  - `transform_data_task`: Triggers dbt transformations

### 3. **Data Transformation** üîÑ
- **Staging Layer** (`stg_weather_data.sql`):
  - Deduplicates records based on timestamp
  - Converts timezone information
  - Cleans and standardizes data formats

- **Mart Layer**:
  - `daily_avg.sql`: Aggregates daily temperature and wind speed averages
  - `weather_report.sql`: Creates clean, analysis-ready weather reports

### 4. **Data Visualization** üìä
- **Tool**: Apache Superset provides interactive dashboards
- **Features**: Charts, filters, and real-time analytics
- **Access**: Web-based interface for business intelligence

## üöÄ Quick Start Guide

### Prerequisites
- **Python 3.8+** (for local development and testing)
- Docker & Docker Compose
- WSL2 (for Windows users)
- **Weatherstack API key** (free account available at [weatherstack.com](https://weatherstack.com/))

### 1. **Environment Setup**

#### **Step 1: Get Your Weatherstack API Key** üîë
1. Visit [https://weatherstack.com/](https://weatherstack.com/)
2. Sign up for a free account
3. Navigate to your dashboard and copy your API Access Key

#### **Step 2: Clone and Configure** 
```bash
# Clone the repository
git clone git@github.com:mel-harc/AtmosData-Pipeline.git
cd AtmosData-Pipeline

# Create environment file in the api-request directory
cd api-request
echo "API_Access_Key=your_actual_weatherstack_api_key" > .env

# Return to project root
cd ..
```

> ‚ö†Ô∏è **Important**: Make sure to replace `your_actual_weatherstack_api_key` with your real API key from Weatherstack!

#### **Step 3: Python Virtual Environment Setup** üêç
```bash
# Create a virtual environment
python -m venv venv

# Activate the virtual environment
# On Windows (PowerShell/CMD):
venv\Scripts\activate

# On Linux/macOS:
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt
```

> üí° **Note**: The virtual environment is needed for local development and testing of the API scripts. The main application runs in Docker containers.

### 2. **Launch the Platform**
```bash
# Start all services
make all
```

### 3. **Access the Services**

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow** | http://localhost:8000 | admin / Your PWD |
| **Superset** | http://localhost:8088 | admin / admin |
| **PostgreSQL** | localhost:5000 | ipman / ipman42 |

### 4. **Verify the Pipeline**
1. Navigate to Airflow UI and enable the `weather-api-dbt-orchestrator` DAG
2. Monitor the pipeline execution in real-time
3. Check the transformed data in PostgreSQL
4. Create visualizations in Superset

## üîß Configuration

### Database Schema
```sql
-- Raw weather data structure
CREATE TABLE dev.raw_weather_data (
    id SERIAL PRIMARY KEY,
    city TEXT,
    temperature FLOAT,
    weather_descriptions TEXT,
    wind_speed FLOAT,
    time TIMESTAMP,
    inserted_at TIMESTAMP DEFAULT NOW(),
    utc_offset TEXT
);
```

### dbt Models
- **Sources**: Define connections to raw data tables
- **Staging**: Data cleaning and standardization
- **Mart**: Business logic and analytics-ready tables

### Airflow Configuration
- **Schedule**: Configurable interval (default: 1 minute)
- **Retry Logic**: Built-in error handling and retries
- **Monitoring**: Real-time pipeline status and logs

## üìä Data Models

### Staging Layer
- **`stg_weather_data`**: Deduplicated and cleaned weather records
- Handles timezone conversions and data quality issues

### Mart Layer
- **`daily_avg`**: Daily aggregations of temperature and wind speed
- **`weather_report`**: Current weather conditions for reporting

## üîç Monitoring & Observability

- **Airflow UI**: Pipeline execution monitoring and debugging
- **dbt Logs**: Transformation process tracking
- **PostgreSQL Logs**: Database performance and query analysis
- **Docker Logs**: Container health and resource utilization

### Development Workflow üîÑ
1. **Make changes** to Python scripts in `api-request/`
2. **Test locally** using the virtual environment
3. **Rebuild containers** if needed: `docker-compose up --build`
4. **Monitor logs** through Airflow UI and container logs

## üõ†Ô∏è Advanced Development

### Adding New Data Sources
1. Create new API client in `api-request/`
2. Update `insert_records.py` for new data structure
3. Add new dbt sources and models
4. Update Airflow DAG for additional tasks

### Scaling Considerations
- **Horizontal Scaling**: Add more Airflow workers
- **Database Optimization**: Implement partitioning for large datasets
- **Caching Strategy**: Leverage Redis for frequently accessed data
- **Monitoring**: Add Prometheus/Grafana for advanced monitoring

## üê≥ Docker Services

| Service | Container | Port | Purpose |
|---------|-----------|------|---------|
| PostgreSQL | `pg-db` | 5000 | Primary database |
| Airflow | `airflow_container` | 8000 | Workflow orchestration |
| dbt | `dbt_container` | - | Data transformations |
| Superset | `superset_app` | 8088 | Data visualization |
| Redis | `superset_cache` | 6379 | Caching layer |

## üîí Security Features

- Containerized environment isolation
- Environment variable management
- Database connection security
- Service-to-service authentication

## üìà Performance Optimization

- **Database Indexing**: Optimized queries for large datasets
- **Connection Pooling**: Efficient database connections
- **Caching**: Redis for improved response times
- **Resource Management**: Docker resource limits and monitoring

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests and documentation
5. Submit a pull request


## üÜò Troubleshooting

### Common Issues

#### **API Key Problems** üîë
- **Error**: "API request failed" or "Unauthorized"
- **Solution**: 
  1. Verify your `.env` file exists in the `api-request/` directory
  2. Check that your Weatherstack API key is correct
  3. Ensure you haven't exceeded your API rate limits (free tier: 1,000 calls/month)
  4. Test your API key directly: `curl "http://api.weatherstack.com/current?access_key=YOUR_KEY&query=FES"`

#### **Container Issues** üê≥
- **Error**: "Port already in use"
- **Solution**: Stop conflicting services or change ports in `docker-compose.yml`

#### **Database Connection Issues** üóÑÔ∏è
- **Error**: "Connection refused"
- **Solution**: Wait for PostgreSQL to fully initialize (may take 1-2 minutes on first run)

### Support Resources

For additional help:
- üìß **Email**: [mohammedelharchi822@gmail.com]
- üí¨ **Site**: http://mel-harc.github.io/

---

<div align="center">

**Built By ‚ù§Ô∏è by Mel-harc**

</div>

